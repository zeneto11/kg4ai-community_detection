{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56a9d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8d81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teste concluído. Saída salva em D:/v1.0/Categorias/nq-train-00-test.jsonl\n"
     ]
    }
   ],
   "source": [
    "input_file = \"D:/v1.0/train-json/nq-train-00.jsonl\"\n",
    "output_file = \"D:/v1.0/Categorias/nq-train-00-test.jsonl\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "pattern_title = re.compile(r\"[?&]title=([^&]+)\")\n",
    "pattern_normal = re.compile(r'id=\"mw-normal-catlinks\".*?<UL>(.*?)</UL>', re.S)\n",
    "pattern_oculta = re.compile(r'id=\"mw-hidden-catlinks\".*?<UL>(.*?)</UL>', re.S)\n",
    "\n",
    "# Regex para pegar apenas os campos grandes que podem ser pesados\n",
    "pattern_html_url = re.compile(\n",
    "    r'\"document_html\"\\s*:\\s*\"((?:\\\\.|[^\"\\\\])*)\"|'\n",
    "    r'\"document_url\"\\s*:\\s*\"([^\"]+)\"'\n",
    ")\n",
    "\n",
    "def extract_html_url(json_line):\n",
    "    document_html = \"\"\n",
    "    document_url = \"\"\n",
    "    \n",
    "    for match in pattern_html_url.finditer(json_line):\n",
    "        if match.group(1):\n",
    "            document_html = bytes(match.group(1), \"utf-8\").decode(\"unicode_escape\")\n",
    "        elif match.group(2):\n",
    "            document_url = match.group(2)\n",
    "    \n",
    "    return document_html, document_url\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f_in, open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    for i, linha in enumerate(f_in):\n",
    "        if i >= 10:  # limita a 10 linhas\n",
    "            break\n",
    "\n",
    "        data = json.loads(linha)\n",
    "        example_id = data.get(\"example_id\", \"\")\n",
    "        html, url = extract_html_url(linha)\n",
    "        title_match = pattern_title.search(url)\n",
    "        title = title_match.group(1) if title_match else \"N/A\"\n",
    "        \n",
    "        # Categorias normais\n",
    "        match_normal = pattern_normal.search(html)\n",
    "        if match_normal:\n",
    "            categorias = re.findall(r'>([^<>]+)</A>', match_normal.group(1))\n",
    "            categorias = [c.replace('\\n',' ').replace('\\t','').strip() for c in categorias]\n",
    "        else:\n",
    "            categorias = []\n",
    "\n",
    "        # Categorias ocultas\n",
    "        match_oculta = pattern_oculta.search(html)\n",
    "        if match_oculta:\n",
    "            categorias_ocultas = re.findall(r'>([^<>]+)</A>', match_oculta.group(1))\n",
    "            categorias_ocultas = [c.replace('\\n',' ').replace('\\t','').strip() for c in categorias_ocultas]\n",
    "        else:\n",
    "            categorias_ocultas = []\n",
    "\n",
    "        output_data = {\n",
    "            \"example_id\": example_id,\n",
    "            \"title\": title,\n",
    "            \"categorias\": categorias,\n",
    "            \"categorias_ocultas\": categorias_ocultas\n",
    "        }\n",
    "        f_out.write(json.dumps(output_data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Teste concluído. Saída salva em {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b90f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo processado: D:/v1.0/train-json\\nq-train-00.jsonl → D:/v1.0/Categorias\\nq-train-00.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-01.jsonl → D:/v1.0/Categorias\\nq-train-01.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-02.jsonl → D:/v1.0/Categorias\\nq-train-02.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-03.jsonl → D:/v1.0/Categorias\\nq-train-03.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-04.jsonl → D:/v1.0/Categorias\\nq-train-04.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-05.jsonl → D:/v1.0/Categorias\\nq-train-05.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-06.jsonl → D:/v1.0/Categorias\\nq-train-06.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-07.jsonl → D:/v1.0/Categorias\\nq-train-07.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-08.jsonl → D:/v1.0/Categorias\\nq-train-08.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-09.jsonl → D:/v1.0/Categorias\\nq-train-09.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-10.jsonl → D:/v1.0/Categorias\\nq-train-10.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-11.jsonl → D:/v1.0/Categorias\\nq-train-11.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-12.jsonl → D:/v1.0/Categorias\\nq-train-12.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-13.jsonl → D:/v1.0/Categorias\\nq-train-13.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-14.jsonl → D:/v1.0/Categorias\\nq-train-14.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-15.jsonl → D:/v1.0/Categorias\\nq-train-15.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-16.jsonl → D:/v1.0/Categorias\\nq-train-16.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-17.jsonl → D:/v1.0/Categorias\\nq-train-17.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-18.jsonl → D:/v1.0/Categorias\\nq-train-18.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-19.jsonl → D:/v1.0/Categorias\\nq-train-19.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-20.jsonl → D:/v1.0/Categorias\\nq-train-20.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-21.jsonl → D:/v1.0/Categorias\\nq-train-21.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-22.jsonl → D:/v1.0/Categorias\\nq-train-22.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-23.jsonl → D:/v1.0/Categorias\\nq-train-23.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-24.jsonl → D:/v1.0/Categorias\\nq-train-24.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-25.jsonl → D:/v1.0/Categorias\\nq-train-25.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-26.jsonl → D:/v1.0/Categorias\\nq-train-26.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-27.jsonl → D:/v1.0/Categorias\\nq-train-27.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-28.jsonl → D:/v1.0/Categorias\\nq-train-28.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-29.jsonl → D:/v1.0/Categorias\\nq-train-29.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-30.jsonl → D:/v1.0/Categorias\\nq-train-30.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-31.jsonl → D:/v1.0/Categorias\\nq-train-31.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-32.jsonl → D:/v1.0/Categorias\\nq-train-32.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-33.jsonl → D:/v1.0/Categorias\\nq-train-33.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-34.jsonl → D:/v1.0/Categorias\\nq-train-34.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-35.jsonl → D:/v1.0/Categorias\\nq-train-35.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-36.jsonl → D:/v1.0/Categorias\\nq-train-36.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-37.jsonl → D:/v1.0/Categorias\\nq-train-37.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-38.jsonl → D:/v1.0/Categorias\\nq-train-38.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-39.jsonl → D:/v1.0/Categorias\\nq-train-39.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-40.jsonl → D:/v1.0/Categorias\\nq-train-40.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-41.jsonl → D:/v1.0/Categorias\\nq-train-41.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-42.jsonl → D:/v1.0/Categorias\\nq-train-42.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-43.jsonl → D:/v1.0/Categorias\\nq-train-43.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-44.jsonl → D:/v1.0/Categorias\\nq-train-44.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-45.jsonl → D:/v1.0/Categorias\\nq-train-45.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-46.jsonl → D:/v1.0/Categorias\\nq-train-46.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-47.jsonl → D:/v1.0/Categorias\\nq-train-47.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-48.jsonl → D:/v1.0/Categorias\\nq-train-48.jsonl\n",
      "Arquivo processado: D:/v1.0/train-json\\nq-train-49.jsonl → D:/v1.0/Categorias\\nq-train-49.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Pastas de entrada e saída\n",
    "input_folder = \"D:/v1.0/train-json\"\n",
    "output_folder = \"D:/v1.0/Categorias\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Padrões regex\n",
    "pattern_title = re.compile(r\"[?&]title=([^&]+)\")\n",
    "pattern_normal = re.compile(r'id=\"mw-normal-catlinks\".*?<UL>(.*?)</UL>', re.S)\n",
    "pattern_oculta = re.compile(r'id=\"mw-hidden-catlinks\".*?<UL>(.*?)</UL>', re.S)\n",
    "\n",
    "# Regex apenas para extrair os campos grandes HTML e URL\n",
    "pattern_html_url = re.compile(\n",
    "    r'\"document_html\"\\s*:\\s*\"((?:\\\\.|[^\"\\\\])*)\"|'\n",
    "    r'\"document_url\"\\s*:\\s*\"([^\"]+)\"'\n",
    ")\n",
    "\n",
    "def extract_html_url(json_line):\n",
    "    document_html = \"\"\n",
    "    document_url = \"\"\n",
    "    \n",
    "    for match in pattern_html_url.finditer(json_line):\n",
    "        if match.group(1):\n",
    "            document_html = bytes(match.group(1), \"utf-8\").decode(\"unicode_escape\")\n",
    "        elif match.group(2):\n",
    "            document_url = match.group(2)\n",
    "    \n",
    "    return document_html, document_url\n",
    "\n",
    "# Itera pelos 50 arquivos\n",
    "for i in range(50):\n",
    "    input_file = os.path.join(input_folder, f\"nq-train-{i:02d}.jsonl\")\n",
    "    output_file = os.path.join(output_folder, f\"nq-train-{i:02d}.jsonl\")\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f_in, open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for linha in f_in:\n",
    "            # Pega example_id direto do JSON\n",
    "            data = json.loads(linha)\n",
    "            example_id = data.get(\"example_id\", \"\")\n",
    "            \n",
    "            # Extrai document_html e document_url via regex otimizado\n",
    "            html, url = extract_html_url(linha)\n",
    "            \n",
    "            # Extrai título\n",
    "            title_match = pattern_title.search(url)\n",
    "            title = title_match.group(1) if title_match else \"N/A\"\n",
    "            \n",
    "            # Categorias normais\n",
    "            match_normal = pattern_normal.search(html)\n",
    "            if match_normal:\n",
    "                categorias = re.findall(r'>([^<>]+)</A>', match_normal.group(1))\n",
    "                categorias = [c.replace('\\n',' ').replace('\\t','').strip() for c in categorias]\n",
    "            else:\n",
    "                categorias = []\n",
    "\n",
    "            # Categorias ocultas\n",
    "            match_oculta = pattern_oculta.search(html)\n",
    "            if match_oculta:\n",
    "                categorias_ocultas = re.findall(r'>([^<>]+)</A>', match_oculta.group(1))\n",
    "                categorias_ocultas = [c.replace('\\n',' ').replace('\\t','').strip() for c in categorias_ocultas]\n",
    "            else:\n",
    "                categorias_ocultas = []\n",
    "\n",
    "            # Monta o JSON de saída\n",
    "            output_data = {\n",
    "                \"example_id\": example_id,\n",
    "                \"title\": title,\n",
    "                \"categorias\": categorias,\n",
    "                \"categorias_ocultas\": categorias_ocultas\n",
    "            }\n",
    "\n",
    "            f_out.write(json.dumps(output_data, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    print(f\"Arquivo processado: {input_file} → {output_file}\")\n",
    "# 309m 51.6s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db928d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "arquivos = sorted(glob.glob(\"D:/v1.0/Categorias/nq-train-*.jsonl\"))\n",
    "\n",
    "with open(\"nq-train-all.jsonl\", \"w\", encoding=\"utf-8\") as saida:\n",
    "    for caminho in arquivos:\n",
    "        with open(caminho, \"r\", encoding=\"utf-8\") as f:\n",
    "            for linha in f:\n",
    "                saida.write(linha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
