{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3feea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c0d60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14605fd0",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b5692c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricStatus:\n",
    "    \"\"\"Enumeration for metric computation status.\"\"\"\n",
    "    COMPUTED = \"âœ… computed\"\n",
    "    APPROXIMATED = \"âš ï¸ approximated\"\n",
    "    FAILED = \"âŒ failed\"\n",
    "    SKIPPED = \"â­ï¸ skipped\"\n",
    "\n",
    "\n",
    "class MetricTimer:\n",
    "    \"\"\"Context manager for timing metric computations.\"\"\"\n",
    "\n",
    "    def __init__(self, metric_name: str, logger: logging.Logger):\n",
    "        self.metric_name = metric_name\n",
    "        self.logger = logger\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        self.logger.info(f\"Computing {self.metric_name}...\")\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end_time = time.time()\n",
    "        duration = self.end_time - self.start_time\n",
    "        if exc_type is None:\n",
    "            self.logger.info(\n",
    "                f\"âœ… {self.metric_name} completed in {(duration)}\")\n",
    "        else:\n",
    "            self.logger.warning(\n",
    "                f\"âŒ {self.metric_name} failed after {(duration)}: {exc_val}\")\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end_time and self.start_time:\n",
    "            return self.end_time - self.start_time\n",
    "        return None\n",
    "\n",
    "\n",
    "class FastDistanceAlgorithms:\n",
    "    \"\"\"Fast approximation algorithms for distance metrics.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def approximate_diameter_2sweep(G: nx.Graph) -> Tuple[int, str]:\n",
    "        \"\"\"\n",
    "        Fast 2-sweep approximation for diameter.\n",
    "        Time: O(n + m) vs O(n^2) exact\n",
    "        \"\"\"\n",
    "        if G.number_of_nodes() <= 1:\n",
    "            return 0, \"exact\"\n",
    "\n",
    "        # First BFS from random node\n",
    "        start_node = random.choice(list(G.nodes()))\n",
    "        distances = nx.single_source_shortest_path_length(G, start_node)\n",
    "\n",
    "        # Find furthest node\n",
    "        furthest_node = max(distances.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "        # Second BFS from furthest node\n",
    "        distances2 = nx.single_source_shortest_path_length(G, furthest_node)\n",
    "        diameter_approx = max(distances2.values())\n",
    "\n",
    "        return diameter_approx, \"2-sweep approximation\"\n",
    "\n",
    "    @staticmethod\n",
    "    def approximate_avg_path_sampling(G: nx.Graph, sample_size: int = 1000) -> Tuple[float, str]:\n",
    "        \"\"\"Sample-based average shortest path length estimation.\"\"\"\n",
    "        nodes = list(G.nodes())\n",
    "        n = len(nodes)\n",
    "\n",
    "        if n <= 1:\n",
    "            return 0.0, \"exact\"\n",
    "\n",
    "        # Adaptive sample size\n",
    "        actual_sample = min(sample_size, max(100, n * n // 1000))\n",
    "        path_lengths = []\n",
    "\n",
    "        for _ in range(actual_sample):\n",
    "            source, target = random.sample(nodes, 2)\n",
    "            try:\n",
    "                path_length = nx.shortest_path_length(G, source, target)\n",
    "                path_lengths.append(path_length)\n",
    "            except nx.NetworkXNoPath:\n",
    "                continue\n",
    "\n",
    "        if not path_lengths:\n",
    "            return float('inf'), \"disconnected\"\n",
    "\n",
    "        return np.mean(path_lengths), f\"sampling ({len(path_lengths)} pairs)\"\n",
    "\n",
    "    @staticmethod\n",
    "    def approximate_radius_sampling(G: nx.Graph, sample_size: int = 100) -> Tuple[int, str]:\n",
    "        \"\"\"Fast radius approximation using node sampling.\"\"\"\n",
    "        nodes = list(G.nodes())\n",
    "        n = len(nodes)\n",
    "\n",
    "        if n <= 1:\n",
    "            return 0, \"exact\"\n",
    "\n",
    "        # Sample potential center nodes\n",
    "        sample_nodes = random.sample(nodes, min(sample_size, n))\n",
    "        min_eccentricity = float('inf')\n",
    "\n",
    "        for center in sample_nodes:\n",
    "            try:\n",
    "                distances = nx.single_source_shortest_path_length(G, center)\n",
    "                eccentricity = max(distances.values())\n",
    "                min_eccentricity = min(min_eccentricity, eccentricity)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        if min_eccentricity == float('inf'):\n",
    "            return float('inf'), \"disconnected\"\n",
    "\n",
    "        return int(min_eccentricity), f\"sampling ({len(sample_nodes)} centers)\"\n",
    "\n",
    "    @staticmethod\n",
    "    def estimate_global_efficiency(G: nx.Graph, sample_size: int = 1000) -> Tuple[float, str]:\n",
    "        \"\"\"Fast global efficiency estimation via sampling.\"\"\"\n",
    "        nodes = list(G.nodes())\n",
    "        n = len(nodes)\n",
    "\n",
    "        if n <= 1:\n",
    "            return 0.0, \"exact\"\n",
    "\n",
    "        efficiency_values = []\n",
    "        actual_sample = min(sample_size, max(100, n * n // 1000))\n",
    "\n",
    "        for _ in range(actual_sample):\n",
    "            source, target = random.sample(nodes, 2)\n",
    "            try:\n",
    "                path_length = nx.shortest_path_length(G, source, target)\n",
    "                if path_length > 0:\n",
    "                    efficiency_values.append(1.0 / path_length)\n",
    "            except nx.NetworkXNoPath:\n",
    "                efficiency_values.append(0.0)\n",
    "\n",
    "        if not efficiency_values:\n",
    "            return 0.0, \"no paths\"\n",
    "\n",
    "        return np.mean(efficiency_values), f\"sampling ({len(efficiency_values)} pairs)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4017f715",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cc4f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "def create_test_graph():\n",
    "    \"\"\"Create test graph with metadata.\"\"\"\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(\"../data/v0.0/df_nq_version0.csv\")\n",
    "\n",
    "    # Convert stringified lists into real Python lists\n",
    "    df[\"cites_ids\"] = df[\"cites_ids\"].apply(ast.literal_eval)\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for _, row in df.iterrows():\n",
    "        src = row[\"id\"]\n",
    "        G.add_node(src, title=row[\"d_properties_document_title\"])\n",
    "        for tgt in row[\"cites_ids\"]:\n",
    "            G.add_edge(src, tgt)\n",
    "\n",
    "    # Graph metadata for reporting\n",
    "    graph_info = {\n",
    "        \"source\": \"NQ (Natural Questions) from Google\",\n",
    "        \"description\": \"Graph constructed from wiki articles citation data in the Natural Questions dataset\"\n",
    "    }\n",
    "\n",
    "    return G, graph_info\n",
    "\n",
    "G, _ = create_test_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c6f1e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Information:\n",
      "- Nodes: 27,017\n",
      "- Edges: 335,276\n",
      "- Directed: True\n"
     ]
    }
   ],
   "source": [
    "percentage = 0.25\n",
    "nodes = list(G.nodes())\n",
    "n = len(nodes)\n",
    "sample_size = max(1, int(n * percentage))\n",
    "\n",
    "nodes_list = list(G.nodes())\n",
    "sample_nodes = random.sample(nodes_list, min(sample_size, len(nodes_list)))\n",
    "sample_graph = G.subgraph(sample_nodes).copy()\n",
    "\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {sample_graph.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {sample_graph.number_of_edges():,}\")\n",
    "print(f\"- Directed: {sample_graph.is_directed()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f365995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Information:\n",
      "- Nodes: 94,675\n",
      "- Edges: 4,900,260\n",
      "- Directed: True\n",
      "Graph Information:\n",
      "- Nodes: 23,668\n",
      "- Edges: 1,722,419\n",
      "- Directed: True\n",
      "- Connected: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sccs = list(nx.strongly_connected_components(G))\n",
    "largest_scc = max(sccs, key=len)\n",
    "analysis_graph = G.subgraph(largest_scc).copy()\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {analysis_graph.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {analysis_graph.number_of_edges():,}\")\n",
    "print(f\"- Directed: {analysis_graph.is_directed()}\")\n",
    "\n",
    "def bfs_sample(G, start, sample_size):\n",
    "    visited = {start}\n",
    "    queue = [start]\n",
    "    while queue and len(visited) < sample_size:\n",
    "        v = queue.pop(0)\n",
    "        for u in G.neighbors(v):\n",
    "            if u not in visited:\n",
    "                visited.add(u)\n",
    "                queue.append(u)\n",
    "                if len(visited) >= sample_size:\n",
    "                    break\n",
    "    return G.subgraph(visited).copy()\n",
    "\n",
    "percentage = 0.25\n",
    "start = random.choice(list(analysis_graph.nodes()))\n",
    "sample_graph = bfs_sample(analysis_graph, start, int(len(analysis_graph) * percentage))\n",
    "\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {sample_graph.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {sample_graph.number_of_edges():,}\")\n",
    "print(f\"- Directed: {sample_graph.is_directed()}\")\n",
    "print(f\"- Connected: {nx.is_connected(sample_graph.to_undirected())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cbf045d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Information:\n",
      "- Nodes: 23,659\n",
      "- Edges: 1,722,304\n",
      "- Directed: True\n"
     ]
    }
   ],
   "source": [
    "sccs = list(nx.strongly_connected_components(sample_graph))\n",
    "largest_scc = max(sccs, key=len)\n",
    "analysis_graph2 = sample_graph.subgraph(largest_scc).copy()\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {analysis_graph2.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {analysis_graph2.number_of_edges():,}\")\n",
    "print(f\"- Directed: {analysis_graph2.is_directed()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ef46dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from networkx.algorithms import approximation as approx\n",
    "\n",
    "print(approx.diameter(analysis_graph.to_undirected()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "609bdc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.032798709005158\n"
     ]
    }
   ],
   "source": [
    "print(nx.average_shortest_path_length(analysis_graph2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fa862f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(nx.radius(analysis_graph2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f10fb043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Information:\n",
      "- Nodes: 14,201\n",
      "- Edges: 1,115,438\n",
      "- Directed: True\n",
      "- Connected: True\n",
      "Graph Information:\n",
      "- Nodes: 14,197\n",
      "- Edges: 1,115,381\n",
      "- Directed: True\n"
     ]
    }
   ],
   "source": [
    "start = random.choice(list(analysis_graph.nodes()))\n",
    "sample_graph = bfs_sample(analysis_graph, start, int(len(analysis_graph) * 0.15))\n",
    "\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {sample_graph.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {sample_graph.number_of_edges():,}\")\n",
    "print(f\"- Directed: {sample_graph.is_directed()}\")\n",
    "print(f\"- Connected: {nx.is_connected(sample_graph.to_undirected())}\")\n",
    "\n",
    "sccs = list(nx.strongly_connected_components(sample_graph))\n",
    "largest_scc = max(sccs, key=len)\n",
    "analysis_graph3 = sample_graph.subgraph(largest_scc).copy()\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {analysis_graph3.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {analysis_graph3.number_of_edges():,}\")\n",
    "print(f\"- Directed: {analysis_graph3.is_directed()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0d1c66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.818837962048066\n"
     ]
    }
   ],
   "source": [
    "print(nx.average_shortest_path_length(analysis_graph3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3158761a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(nx.radius(analysis_graph3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7ac7d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46832696495964377\n"
     ]
    }
   ],
   "source": [
    "print(nx.global_efficiency(analysis_graph3.to_undirected()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7f012e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Information:\n",
      "- Nodes: 18,935\n",
      "- Edges: 1,423,749\n",
      "- Directed: True\n",
      "- Connected: True\n",
      "Graph Information:\n",
      "- Nodes: 18,933\n",
      "- Edges: 1,423,737\n",
      "- Directed: True\n"
     ]
    }
   ],
   "source": [
    "start = random.choice(list(analysis_graph.nodes()))\n",
    "sample_graph = bfs_sample(analysis_graph, start, int(len(analysis_graph) * 0.20))\n",
    "\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {sample_graph.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {sample_graph.number_of_edges():,}\")\n",
    "print(f\"- Directed: {sample_graph.is_directed()}\")\n",
    "print(f\"- Connected: {nx.is_connected(sample_graph.to_undirected())}\")\n",
    "\n",
    "sccs = list(nx.strongly_connected_components(sample_graph))\n",
    "largest_scc = max(sccs, key=len)\n",
    "analysis_graph4 = sample_graph.subgraph(largest_scc).copy()\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {analysis_graph4.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {analysis_graph4.number_of_edges():,}\")\n",
    "print(f\"- Directed: {analysis_graph4.is_directed()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "772d0d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.998498633895194\n"
     ]
    }
   ],
   "source": [
    "print(nx.average_shortest_path_length(analysis_graph4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67145a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(nx.radius(analysis_graph4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d382891f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4627325813771752\n"
     ]
    }
   ],
   "source": [
    "print(nx.global_efficiency(analysis_graph4.to_undirected()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d93318ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Information:\n",
      "- Nodes: 4,733\n",
      "- Edges: 397,460\n",
      "- Directed: True\n",
      "- Connected: True\n",
      "Graph Information:\n",
      "- Nodes: 4,729\n",
      "- Edges: 397,450\n",
      "- Directed: True\n"
     ]
    }
   ],
   "source": [
    "start = random.choice(list(analysis_graph.nodes()))\n",
    "sample_graph = bfs_sample(analysis_graph, start, int(len(analysis_graph) * 0.05))\n",
    "\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {sample_graph.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {sample_graph.number_of_edges():,}\")\n",
    "print(f\"- Directed: {sample_graph.is_directed()}\")\n",
    "print(f\"- Connected: {nx.is_connected(sample_graph.to_undirected())}\")\n",
    "\n",
    "sccs = list(nx.strongly_connected_components(sample_graph))\n",
    "largest_scc = max(sccs, key=len)\n",
    "analysis_graph5 = sample_graph.subgraph(largest_scc).copy()\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {analysis_graph5.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {analysis_graph5.number_of_edges():,}\")\n",
    "print(f\"- Directed: {analysis_graph5.is_directed()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d82037aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5717285950997537\n"
     ]
    }
   ],
   "source": [
    "print(nx.average_shortest_path_length(analysis_graph5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7170d4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(nx.radius(analysis_graph5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5729ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49387526586820557\n"
     ]
    }
   ],
   "source": [
    "print(nx.global_efficiency(analysis_graph5.to_undirected()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76d0a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Information:\n",
      "- Nodes: 108,071\n",
      "- Edges: 5,122,983\n",
      "- Directed: True\n",
      "- Source: NQ (Natural Questions) from Google\n",
      "- Description: Graph constructed from wiki articles citation data in the Natural Questions dataset\n",
      "\n",
      "Connectivity Analysis:\n",
      "- Is strongly connected: False\n",
      "- Number of components: 13288\n",
      "- Largest component size: 94,675 nodes\n",
      "- Largest component ratio: 0.8760\n",
      "\n",
      "Working with largest component: 94,675 nodes, 4,900,260 edges\n"
     ]
    }
   ],
   "source": [
    "# Extract and analyze the graph\n",
    "G_graph, graph_info = G\n",
    "print(f\"Graph Information:\")\n",
    "print(f\"- Nodes: {G_graph.number_of_nodes():,}\")\n",
    "print(f\"- Edges: {G_graph.number_of_edges():,}\")\n",
    "print(f\"- Directed: {G_graph.is_directed()}\")\n",
    "print(f\"- Source: {graph_info['source']}\")\n",
    "print(f\"- Description: {graph_info['description']}\")\n",
    "\n",
    "# Check connectivity and components\n",
    "if G_graph.is_directed():\n",
    "    is_connected = nx.is_strongly_connected(G_graph)\n",
    "    components = list(nx.strongly_connected_components(G_graph))\n",
    "    conn_type = \"strongly connected\"\n",
    "else:\n",
    "    is_connected = nx.is_connected(G_graph)\n",
    "    components = list(nx.connected_components(G_graph))\n",
    "    conn_type = \"connected\"\n",
    "\n",
    "print(f\"\\nConnectivity Analysis:\")\n",
    "print(f\"- Is {conn_type}: {is_connected}\")\n",
    "print(f\"- Number of components: {len(components)}\")\n",
    "\n",
    "if components:\n",
    "    component_sizes = [len(comp) for comp in components]\n",
    "    largest_component_size = max(component_sizes)\n",
    "    print(f\"- Largest component size: {largest_component_size:,} nodes\")\n",
    "    print(f\"- Largest component ratio: {largest_component_size/G_graph.number_of_nodes():.4f}\")\n",
    "    \n",
    "    # Store the largest component for experiments\n",
    "    largest_component = max(components, key=len)\n",
    "    G_main = G_graph.subgraph(largest_component).copy()\n",
    "    print(f\"\\nWorking with largest component: {G_main.number_of_nodes():,} nodes, {G_main.number_of_edges():,} edges\")\n",
    "else:\n",
    "    G_main = G_graph\n",
    "    print(\"Working with full graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235852cc",
   "metadata": {},
   "source": [
    "## Sample Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87cb8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleSizeExperiment:\n",
    "    \"\"\"Framework for testing optimal sample sizes for distance metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, graph: nx.Graph, max_time_threshold: float = 300.0):\n",
    "        \"\"\"\n",
    "        Initialize experiment framework.\n",
    "        \n",
    "        Args:\n",
    "            graph: NetworkX graph for experiments\n",
    "            max_time_threshold: Maximum acceptable computation time in seconds\n",
    "        \"\"\"\n",
    "        self.graph = graph\n",
    "        self.max_time_threshold = max_time_threshold\n",
    "        self.results = []\n",
    "        self.fast_algo = FastDistanceAlgorithms()\n",
    "        \n",
    "        # Base graph properties\n",
    "        self.total_nodes = graph.number_of_nodes()\n",
    "        self.total_edges = graph.number_of_edges()\n",
    "        \n",
    "        print(f\"Experiment Setup:\")\n",
    "        print(f\"- Graph: {self.total_nodes:,} nodes, {self.total_edges:,} edges\")\n",
    "        print(f\"- Max time threshold: {max_time_threshold:.1f} seconds\")\n",
    "        \n",
    "    def generate_sample_sizes(self, start_samples=100, max_samples=10000, step_multiplier=1.5):\n",
    "        \"\"\"Generate increasing sample sizes for testing.\"\"\"\n",
    "        sample_sizes = []\n",
    "        current = start_samples\n",
    "        \n",
    "        while current <= max_samples:\n",
    "            sample_sizes.append(int(current))\n",
    "            current *= step_multiplier\n",
    "            \n",
    "        # Add a few more aggressive sizes if graph is large enough\n",
    "        if self.total_nodes > 50000:\n",
    "            additional_sizes = [15000, 20000, 25000, 30000]\n",
    "            sample_sizes.extend([s for s in additional_sizes if s <= max_samples])\n",
    "            \n",
    "        return sorted(set(sample_sizes))\n",
    "    \n",
    "    def test_metric_scalability(self, metric_name: str, metric_func, sample_sizes: List[int], \n",
    "                              num_runs: int = 3, **func_kwargs):\n",
    "        \"\"\"\n",
    "        Test a specific metric across different sample sizes.\n",
    "        \n",
    "        Args:\n",
    "            metric_name: Name of the metric being tested\n",
    "            metric_func: Function to compute the metric\n",
    "            sample_sizes: List of sample sizes to test\n",
    "            num_runs: Number of runs per sample size for averaging\n",
    "            **func_kwargs: Additional kwargs for the metric function\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing {metric_name.upper()} scalability\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        metric_results = []\n",
    "        nodes_list = list(self.graph.nodes())\n",
    "        \n",
    "        for sample_size in sample_sizes:\n",
    "            print(f\"\\nTesting sample size: {sample_size:,} ({sample_size/self.total_nodes:.4f} of total)\")\n",
    "            \n",
    "            run_times = []\n",
    "            run_values = []\n",
    "            run_methods = []\n",
    "            \n",
    "            for run in range(num_runs):\n",
    "                print(f\"  Run {run + 1}/{num_runs}...\", end=\" \")\n",
    "                \n",
    "                try:\n",
    "                    # Create sample subgraph\n",
    "                    sample_nodes = random.sample(nodes_list, min(sample_size, len(nodes_list)))\n",
    "                    sample_graph = self.graph.subgraph(sample_nodes).copy()\n",
    "                    \n",
    "                    # Make sure sample is connected (take largest component)\n",
    "                    if sample_graph.is_directed():\n",
    "                        if not nx.is_strongly_connected(sample_graph):\n",
    "                            components = list(nx.strongly_connected_components(sample_graph))\n",
    "                            if components:\n",
    "                                largest_comp = max(components, key=len)\n",
    "                                sample_graph = sample_graph.subgraph(largest_comp).copy()\n",
    "                    else:\n",
    "                        if not nx.is_connected(sample_graph):\n",
    "                            components = list(nx.connected_components(sample_graph))\n",
    "                            if components:\n",
    "                                largest_comp = max(components, key=len)\n",
    "                                sample_graph = sample_graph.subgraph(largest_comp).copy()\n",
    "                    \n",
    "                    # Convert to undirected for distance calculations\n",
    "                    distance_graph = sample_graph.to_undirected() if sample_graph.is_directed() else sample_graph\n",
    "                    \n",
    "                    # Time the computation\n",
    "                    start_time = time.time()\n",
    "                    result = metric_func(distance_graph, **func_kwargs)\n",
    "                    end_time = time.time()\n",
    "                    \n",
    "                    computation_time = end_time - start_time\n",
    "                    run_times.append(computation_time)\n",
    "                    \n",
    "                    # Handle different return types\n",
    "                    if isinstance(result, tuple):\n",
    "                        value, method = result\n",
    "                        run_values.append(value)\n",
    "                        run_methods.append(method)\n",
    "                    else:\n",
    "                        run_values.append(result)\n",
    "                        run_methods.append(\"exact\")\n",
    "                    \n",
    "                    print(f\"âœ… {computation_time:.2f}s\")\n",
    "                    \n",
    "                    # Early termination if taking too long\n",
    "                    if computation_time > self.max_time_threshold:\n",
    "                        print(f\"    âš ï¸ Exceeded time threshold ({self.max_time_threshold}s), stopping here\")\n",
    "                        break\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error: {str(e)[:50]}\")\n",
    "                    continue\n",
    "            \n",
    "            if run_times:\n",
    "                # Calculate statistics\n",
    "                avg_time = np.mean(run_times)\n",
    "                std_time = np.std(run_times) if len(run_times) > 1 else 0\n",
    "                avg_value = np.mean(run_values) if run_values else None\n",
    "                std_value = np.std(run_values) if len(run_values) > 1 else 0\n",
    "                \n",
    "                result_entry = {\n",
    "                    'metric': metric_name,\n",
    "                    'sample_size': sample_size,\n",
    "                    'sample_ratio': sample_size / self.total_nodes,\n",
    "                    'actual_nodes_tested': sample_size,  # Could be less if component extraction\n",
    "                    'num_successful_runs': len(run_times),\n",
    "                    'avg_computation_time': avg_time,\n",
    "                    'std_computation_time': std_time,\n",
    "                    'avg_metric_value': avg_value,\n",
    "                    'std_metric_value': std_value,\n",
    "                    'method': run_methods[0] if run_methods else \"unknown\",\n",
    "                    'exceeded_threshold': avg_time > self.max_time_threshold\n",
    "                }\n",
    "                \n",
    "                metric_results.append(result_entry)\n",
    "                self.results.append(result_entry)\n",
    "                \n",
    "                print(f\"    ðŸ“Š Avg time: {avg_time:.2f}Â±{std_time:.2f}s, Value: {avg_value:.4f}Â±{std_value:.4f}\")\n",
    "                \n",
    "                # Stop if consistently exceeding threshold\n",
    "                if avg_time > self.max_time_threshold:\n",
    "                    print(f\"    ðŸ›‘ Sample size {sample_size} exceeds threshold, stopping metric testing\")\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"    âŒ No successful runs for sample size {sample_size}\")\n",
    "        \n",
    "        return metric_results\n",
    "    \n",
    "    def find_optimal_sample_size(self, metric_results: List[Dict], time_budget: float = None) -> Dict:\n",
    "        \"\"\"Find the optimal sample size for a metric based on time constraints.\"\"\"\n",
    "        if not metric_results:\n",
    "            return {}\n",
    "        \n",
    "        budget = time_budget or self.max_time_threshold\n",
    "        \n",
    "        # Filter results within time budget\n",
    "        viable_results = [r for r in metric_results if r['avg_computation_time'] <= budget]\n",
    "        \n",
    "        if not viable_results:\n",
    "            return {\"error\": \"No sample sizes within time budget\"}\n",
    "        \n",
    "        # Find largest viable sample size\n",
    "        optimal = max(viable_results, key=lambda x: x['sample_size'])\n",
    "        \n",
    "        return {\n",
    "            'metric': optimal['metric'],\n",
    "            'optimal_sample_size': optimal['sample_size'],\n",
    "            'optimal_sample_ratio': optimal['sample_ratio'],\n",
    "            'expected_computation_time': optimal['avg_computation_time'],\n",
    "            'expected_metric_value': optimal['avg_metric_value'],\n",
    "            'method': optimal['method'],\n",
    "            'confidence': f\"Â±{optimal['std_metric_value']:.4f}\" if optimal['std_metric_value'] else \"N/A\"\n",
    "        }\n",
    "    \n",
    "    def save_results(self, filename: str = None):\n",
    "        \"\"\"Save experimental results to JSON file.\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"sample_size_experiment_{timestamp}.json\"\n",
    "        \n",
    "        # Prepare data for JSON serialization\n",
    "        export_data = {\n",
    "            'experiment_metadata': {\n",
    "                'total_nodes': self.total_nodes,\n",
    "                'total_edges': self.total_edges,\n",
    "                'max_time_threshold': self.max_time_threshold,\n",
    "                'experiment_date': datetime.now().isoformat(),\n",
    "                'graph_type': 'directed' if self.graph.is_directed() else 'undirected'\n",
    "            },\n",
    "            'results': self.results\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        output_path = Path(\"../community_detection/output\") / filename\n",
    "        output_path.parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2, default=str)\n",
    "        \n",
    "        print(f\"Results saved to: {output_path}\")\n",
    "        return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99d50394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Setup:\n",
      "- Graph: 94,675 nodes, 4,900,260 edges\n",
      "- Max time threshold: 600.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Initialize experiment\n",
    "experiment = SampleSizeExperiment(G_main, max_time_threshold=600.0)  # 10 minutes max per test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61e766",
   "metadata": {},
   "source": [
    "## Initial Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d7ab61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample sizes to test:\n",
      "  100 nodes (0.0011 of total)\n",
      "  180 nodes (0.0019 of total)\n",
      "  324 nodes (0.0034 of total)\n",
      "  583 nodes (0.0062 of total)\n",
      "  1,049 nodes (0.0111 of total)\n",
      "  1,889 nodes (0.0200 of total)\n",
      "  3,401 nodes (0.0359 of total)\n",
      "  6,122 nodes (0.0647 of total)\n",
      "\n",
      "Total tests planned: 8 sample sizes Ã— 4 metrics Ã— 3 runs = 96 individual runs\n"
     ]
    }
   ],
   "source": [
    "# Define sample sizes to test (progressive scaling)\n",
    "sample_sizes = experiment.generate_sample_sizes(\n",
    "    start_samples=100, \n",
    "    max_samples=8000,  # Conservative max for initial testing\n",
    "    step_multiplier=1.8\n",
    ")\n",
    "\n",
    "print(\"Sample sizes to test:\")\n",
    "for size in sample_sizes:\n",
    "    ratio = size / experiment.total_nodes\n",
    "    print(f\"  {size:,} nodes ({ratio:.4f} of total)\")\n",
    "\n",
    "print(f\"\\nTotal tests planned: {len(sample_sizes)} sample sizes Ã— 4 metrics Ã— 3 runs = {len(sample_sizes) * 4 * 3} individual runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bdb362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting DIAMETER experiments...\n",
      "\n",
      "============================================================\n",
      "Testing DIAMETER scalability\n",
      "============================================================\n",
      "\n",
      "Testing sample size: 100 (0.0011 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 0.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 180 (0.0019 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 1.3333Â±0.4714\n",
      "\n",
      "Testing sample size: 324 (0.0034 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 2.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 583 (0.0062 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 2.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 1,049 (0.0111 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 5.6667Â±2.0548\n",
      "\n",
      "Testing sample size: 1,889 (0.0200 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.02s\n",
      "  Run 3/3... âœ… 0.02s\n",
      "    ðŸ“Š Avg time: 0.01Â±0.01s, Value: 11.6667Â±2.4944\n",
      "\n",
      "Testing sample size: 3,401 (0.0359 of total)\n",
      "  Run 1/3... âœ… 0.24s\n",
      "  Run 2/3... âœ… 0.26s\n",
      "  Run 3/3... âœ… 0.22s\n",
      "    ðŸ“Š Avg time: 0.24Â±0.02s, Value: 9.6667Â±0.4714\n",
      "\n",
      "Testing sample size: 6,122 (0.0647 of total)\n",
      "  Run 1/3... âœ… 5.43s\n",
      "  Run 2/3... âœ… 1.96s\n",
      "  Run 3/3... âœ… 7.41s\n",
      "    ðŸ“Š Avg time: 4.93Â±2.25s, Value: 10.0000Â±0.8165\n"
     ]
    }
   ],
   "source": [
    "# 1. Test DIAMETER metric (2-sweep approximation)\n",
    "print(\"ðŸš€ Starting DIAMETER experiments...\")\n",
    "diameter_results = experiment.test_metric_scalability(\n",
    "    metric_name=\"diameter\",\n",
    "    metric_func=nx.diameter,\n",
    "    sample_sizes=sample_sizes,\n",
    "    num_runs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8213b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting RADIUS experiments...\n",
      "\n",
      "============================================================\n",
      "Testing RADIUS scalability\n",
      "============================================================\n",
      "\n",
      "Testing sample size: 100 (0.0011 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 1.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 180 (0.0019 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 1.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 324 (0.0034 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 1.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 583 (0.0062 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 1.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 1,049 (0.0111 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 2.3333Â±1.2472\n",
      "\n",
      "Testing sample size: 1,889 (0.0200 of total)\n",
      "  Run 1/3... âœ… 0.01s\n",
      "  Run 2/3... âœ… 0.01s\n",
      "  Run 3/3... âœ… 0.01s\n",
      "    ðŸ“Š Avg time: 0.01Â±0.00s, Value: 5.0000Â±0.8165\n",
      "\n",
      "Testing sample size: 3,401 (0.0359 of total)\n",
      "  Run 1/3... âœ… 0.43s\n",
      "  Run 2/3... âœ… 0.22s\n",
      "  Run 3/3... âœ… 0.37s\n",
      "    ðŸ“Š Avg time: 0.34Â±0.09s, Value: 5.6667Â±0.4714\n",
      "\n",
      "Testing sample size: 6,122 (0.0647 of total)\n",
      "  Run 1/3... âœ… 6.44s\n",
      "  Run 2/3... âœ… 3.73s\n",
      "  Run 3/3... âœ… 5.06s\n",
      "    ðŸ“Š Avg time: 5.08Â±1.10s, Value: 5.6667Â±0.4714\n"
     ]
    }
   ],
   "source": [
    "# 2. Test RADIUS metric (sampling approximation)\n",
    "print(\"ðŸš€ Starting RADIUS experiments...\")\n",
    "radius_results = experiment.test_metric_scalability(\n",
    "    metric_name=\"radius\",\n",
    "    metric_func=nx.radius,\n",
    "    sample_sizes=sample_sizes,\n",
    "    num_runs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea0d4e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting AVERAGE DISTANCE experiments...\n",
      "\n",
      "============================================================\n",
      "Testing AVG_DISTANCE scalability\n",
      "============================================================\n",
      "\n",
      "Testing sample size: 100 (0.0011 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 0.6667Â±0.4714\n",
      "\n",
      "Testing sample size: 180 (0.0019 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 1.2222Â±0.3143\n",
      "\n",
      "Testing sample size: 324 (0.0034 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 1.1111Â±0.1571\n",
      "\n",
      "Testing sample size: 583 (0.0062 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 1.7000Â±0.1414\n",
      "\n",
      "Testing sample size: 1,049 (0.0111 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 3.0159Â±0.9290\n",
      "\n",
      "Testing sample size: 1,889 (0.0200 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.01s\n",
      "    ðŸ“Š Avg time: 0.01Â±0.00s, Value: 3.9848Â±0.2096\n",
      "\n",
      "Testing sample size: 3,401 (0.0359 of total)\n",
      "  Run 1/3... âœ… 0.34s\n",
      "  Run 2/3... âœ… 0.25s\n",
      "  Run 3/3... âœ… 0.26s\n",
      "    ðŸ“Š Avg time: 0.28Â±0.04s, Value: 4.2861Â±0.1450\n",
      "\n",
      "Testing sample size: 6,122 (0.0647 of total)\n",
      "  Run 1/3... âœ… 2.97s\n",
      "  Run 2/3... âœ… 4.92s\n",
      "  Run 3/3... âœ… 3.81s\n",
      "    ðŸ“Š Avg time: 3.90Â±0.80s, Value: 3.6124Â±0.6269\n"
     ]
    }
   ],
   "source": [
    "# 3. Test AVERAGE SHORTEST PATH LENGTH metric (sampling approximation)\n",
    "print(\"ðŸš€ Starting AVERAGE DISTANCE experiments...\")\n",
    "avg_distance_results = experiment.test_metric_scalability(\n",
    "    metric_name=\"avg_distance\",\n",
    "    metric_func=nx.average_shortest_path_length,\n",
    "    sample_sizes=sample_sizes,\n",
    "    num_runs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fa1cb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting GLOBAL EFFICIENCY experiments...\n",
      "\n",
      "============================================================\n",
      "Testing GLOBAL_EFFICIENCY scalability\n",
      "============================================================\n",
      "\n",
      "Testing sample size: 100 (0.0011 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 0.3333Â±0.4714\n",
      "\n",
      "Testing sample size: 180 (0.0019 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 0.6667Â±0.4714\n",
      "\n",
      "Testing sample size: 324 (0.0034 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 0.8426Â±0.1249\n",
      "\n",
      "Testing sample size: 583 (0.0062 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 0.7351Â±0.0725\n",
      "\n",
      "Testing sample size: 1,049 (0.0111 of total)\n",
      "  Run 1/3... âœ… 0.00s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.00s\n",
      "    ðŸ“Š Avg time: 0.00Â±0.00s, Value: 0.5667Â±0.1123\n",
      "\n",
      "Testing sample size: 1,889 (0.0200 of total)\n",
      "  Run 1/3... âœ… 0.01s\n",
      "  Run 2/3... âœ… 0.00s\n",
      "  Run 3/3... âœ… 0.01s\n",
      "    ðŸ“Š Avg time: 0.01Â±0.00s, Value: 0.2806Â±0.0088\n",
      "\n",
      "Testing sample size: 3,401 (0.0359 of total)\n",
      "  Run 1/3... âœ… 0.28s\n",
      "  Run 2/3... âœ… 0.27s\n",
      "  Run 3/3... âœ… 0.27s\n",
      "    ðŸ“Š Avg time: 0.27Â±0.01s, Value: 0.2636Â±0.0094\n",
      "\n",
      "Testing sample size: 6,122 (0.0647 of total)\n",
      "  Run 1/3... âœ… 4.89s\n",
      "  Run 2/3... âœ… 4.84s\n",
      "  Run 3/3... âœ… 4.87s\n",
      "    ðŸ“Š Avg time: 4.86Â±0.02s, Value: 0.2616Â±0.0060\n"
     ]
    }
   ],
   "source": [
    "# 4. Test GLOBAL EFFICIENCY metric (sampling approximation)\n",
    "print(\"ðŸš€ Starting GLOBAL EFFICIENCY experiments...\")\n",
    "efficiency_results = experiment.test_metric_scalability(\n",
    "    metric_name=\"global_efficiency\",\n",
    "    metric_func=nx.global_efficiency,\n",
    "    sample_sizes=sample_sizes,\n",
    "    num_runs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd723d",
   "metadata": {},
   "source": [
    "## Larger Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc02338b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing larger sample sizes to find performance limits...\n",
      "Extended sample sizes:\n",
      "  10,000 nodes (0.1056 of total)\n",
      "  15,000 nodes (0.1584 of total)\n",
      "  20,000 nodes (0.2112 of total)\n",
      "  25,000 nodes (0.2641 of total)\n",
      "  30,000 nodes (0.3169 of total)\n",
      "  40,000 nodes (0.4225 of total)\n",
      "  50,000 nodes (0.5281 of total)\n",
      "  60,000 nodes (0.6337 of total)\n"
     ]
    }
   ],
   "source": [
    "# Since the initial tests were fast, let's test with much larger sample sizes\n",
    "larger_sample_sizes = [10000, 15000, 20000, 25000, 30000, 40000, 50000, 60000]\n",
    "\n",
    "print(\"ðŸ” Testing larger sample sizes to find performance limits...\")\n",
    "print(\"Extended sample sizes:\")\n",
    "for size in larger_sample_sizes:\n",
    "    ratio = size / experiment.total_nodes\n",
    "    print(f\"  {size:,} nodes ({ratio:.4f} of total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fea38e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Testing GLOBAL EFFICIENCY with larger sample sizes...\n",
      "\n",
      "============================================================\n",
      "Testing GLOBAL_EFFICIENCY_LARGE scalability\n",
      "============================================================\n",
      "\n",
      "Testing sample size: 10,000 (0.1056 of total)\n",
      "  Run 1/1... âœ… 35.73s\n",
      "    ðŸ“Š Avg time: 35.73Â±0.00s, Value: 0.3795Â±0.0000\n",
      "\n",
      "Testing sample size: 15,000 (0.1584 of total)\n",
      "  Run 1/1... âœ… 157.40s\n",
      "    ðŸ“Š Avg time: 157.40Â±0.00s, Value: 0.3129Â±0.0000\n",
      "\n",
      "Testing sample size: 20,000 (0.2112 of total)\n",
      "  Run 1/1... âœ… 411.81s\n",
      "    ðŸ“Š Avg time: 411.81Â±0.00s, Value: 0.3706Â±0.0000\n",
      "\n",
      "Testing sample size: 25,000 (0.2641 of total)\n",
      "  Run 1/1... âœ… 847.99s\n",
      "    âš ï¸ Exceeded time threshold (600.0s), stopping here\n",
      "    ðŸ“Š Avg time: 847.99Â±0.00s, Value: 0.3645Â±0.0000\n",
      "    ðŸ›‘ Sample size 25000 exceeds threshold, stopping metric testing\n"
     ]
    }
   ],
   "source": [
    "# Test the most challenging metric first - global efficiency with larger sizes\n",
    "print(\"\\nðŸš€ Testing GLOBAL EFFICIENCY with larger sample sizes...\")\n",
    "efficiency_large_results = experiment.test_metric_scalability(\n",
    "    metric_name=\"global_efficiency_large\",\n",
    "    metric_func=nx.global_efficiency,\n",
    "    sample_sizes=larger_sample_sizes,\n",
    "    num_runs=1  # Reduce runs for larger tests\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f3d707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Testing AVERAGE DISTANCE with larger sample sizes...\n",
      "\n",
      "============================================================\n",
      "Testing AVG_DISTANCE_LARGE scalability\n",
      "============================================================\n",
      "\n",
      "Testing sample size: 10,000 (0.1056 of total)\n",
      "  Run 1/2... âœ… 29.01s\n",
      "  Run 2/2... âœ… 35.62s\n",
      "    ðŸ“Š Avg time: 32.31Â±3.30s, Value: 3.4741Â±0.0037\n",
      "\n",
      "Testing sample size: 15,000 (0.1584 of total)\n",
      "  Run 1/2... âœ… 156.05s\n",
      "  Run 2/2... âœ… 147.94s\n",
      "    ðŸ“Š Avg time: 151.99Â±4.05s, Value: 3.1903Â±0.2553\n",
      "\n",
      "Testing sample size: 20,000 (0.2112 of total)\n",
      "  Run 1/2... âœ… 413.71s\n",
      "  Run 2/2... âœ… 387.77s\n",
      "    ðŸ“Š Avg time: 400.74Â±12.97s, Value: 3.0002Â±0.4213\n",
      "\n",
      "Testing sample size: 25,000 (0.2641 of total)\n",
      "  Run 1/2... âœ… 847.01s\n",
      "    âš ï¸ Exceeded time threshold (600.0s), stopping here\n",
      "    ðŸ“Š Avg time: 847.01Â±0.00s, Value: 2.8415Â±0.0000\n",
      "    ðŸ›‘ Sample size 25000 exceeds threshold, stopping metric testing\n"
     ]
    }
   ],
   "source": [
    "# Test AVERAGE DISTANCE with larger sample sizes\n",
    "print(\"ðŸš€ Testing AVERAGE DISTANCE with larger sample sizes...\")\n",
    "avg_distance_large_results = experiment.test_metric_scalability(\n",
    "    metric_name=\"avg_distance_large\",\n",
    "    metric_func=nx.average_shortest_path_length,\n",
    "    sample_sizes=larger_sample_sizes,\n",
    "    num_runs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b593a108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Testing RADIUS with larger sample sizes...\n",
      "\n",
      "============================================================\n",
      "Testing RADIUS_LARGE scalability\n",
      "============================================================\n",
      "\n",
      "Testing sample size: 10,000 (0.1056 of total)\n",
      "  Run 1/2... âœ… 33.77s\n",
      "  Run 2/2... âœ… 36.12s\n",
      "    ðŸ“Š Avg time: 34.94Â±1.18s, Value: 5.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 15,000 (0.1584 of total)\n",
      "  Run 1/2... âœ… 145.93s\n",
      "  Run 2/2... âœ… 158.86s\n",
      "    ðŸ“Š Avg time: 152.39Â±6.47s, Value: 3.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 20,000 (0.2112 of total)\n",
      "  Run 1/2... âœ… 374.94s\n",
      "  Run 2/2... âœ… 416.92s\n",
      "    ðŸ“Š Avg time: 395.93Â±20.99s, Value: 4.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 25,000 (0.2641 of total)\n",
      "  Run 1/2... âœ… 894.17s\n",
      "    âš ï¸ Exceeded time threshold (600.0s), stopping here\n",
      "    ðŸ“Š Avg time: 894.17Â±0.00s, Value: 4.0000Â±0.0000\n",
      "    ðŸ›‘ Sample size 25000 exceeds threshold, stopping metric testing\n"
     ]
    }
   ],
   "source": [
    "# Test RADIUS with larger sample sizes\n",
    "print(\"ðŸš€ Testing RADIUS with larger sample sizes...\")\n",
    "radius_large_results = experiment.test_metric_scalability(\n",
    "    metric_name=\"radius_large\",\n",
    "    metric_func=nx.radius,\n",
    "    sample_sizes=larger_sample_sizes,\n",
    "    num_runs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bf17f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Testing DIAMETER with larger sample sizes...\n",
      "\n",
      "============================================================\n",
      "Testing DIAMETER_LARGE scalability\n",
      "============================================================\n",
      "\n",
      "Testing sample size: 10,000 (0.1056 of total)\n",
      "  Run 1/2... âœ… 33.52s\n",
      "  Run 2/2... âœ… 35.79s\n",
      "    ðŸ“Š Avg time: 34.65Â±1.14s, Value: 8.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 15,000 (0.1584 of total)\n",
      "  Run 1/2... âœ… 155.88s\n",
      "  Run 2/2... âœ… 160.89s\n",
      "    ðŸ“Š Avg time: 158.38Â±2.51s, Value: 8.5000Â±0.5000\n",
      "\n",
      "Testing sample size: 20,000 (0.2112 of total)\n",
      "  Run 1/2... âœ… 411.52s\n",
      "  Run 2/2... âœ… 421.95s\n",
      "    ðŸ“Š Avg time: 416.73Â±5.21s, Value: 8.0000Â±0.0000\n",
      "\n",
      "Testing sample size: 25,000 (0.2641 of total)\n",
      "  Run 1/2... âœ… 850.99s\n",
      "    âš ï¸ Exceeded time threshold (600.0s), stopping here\n",
      "    ðŸ“Š Avg time: 850.99Â±0.00s, Value: 6.0000Â±0.0000\n",
      "    ðŸ›‘ Sample size 25000 exceeds threshold, stopping metric testing\n"
     ]
    }
   ],
   "source": [
    "# Test DIAMETER with larger sample sizes\n",
    "print(\"ðŸš€ Testing DIAMETER with larger sample sizes...\")\n",
    "diameter_large_results = experiment.test_metric_scalability(\n",
    "    metric_name=\"diameter_large\",\n",
    "    metric_func=nx.diameter,\n",
    "    sample_sizes=larger_sample_sizes,\n",
    "    num_runs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a4b8f6",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results and create final recommendations\n",
    "results_file = experiment.save_results(\"sample_size_optimization_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sample_size_optimization(results_data):\n",
    "    \"\"\"\n",
    "    Analyze graph metric computation results to find optimal sample sizes\n",
    "    for different time budgets and provide clear recommendations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load results\n",
    "    if isinstance(results_data, str):\n",
    "        with open(results_data, 'r') as f:\n",
    "            data = json.load(f)\n",
    "    else:\n",
    "        data = results_data\n",
    "    \n",
    "    results = data['results']\n",
    "    metadata = data['experiment_metadata']\n",
    "    \n",
    "    print(\"ðŸ—ï¸  GRAPH STRUCTURE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total nodes: {metadata['total_nodes']:,}\")\n",
    "    print(f\"Total edges: {metadata['total_edges']:,}\")\n",
    "    print(f\"Graph type: {metadata['graph_type']}\")\n",
    "    print(f\"Max time threshold: {metadata['max_time_threshold']}s\")\n",
    "    print(f\"Experiment date: {metadata['experiment_date']}\")\n",
    "    \n",
    "    # Group results by metric\n",
    "    metrics = defaultdict(list)\n",
    "    for result in results:\n",
    "        base_metric = result['metric'].replace('_large', '')\n",
    "        metrics[base_metric].append(result)\n",
    "    \n",
    "    # Sort each metric's results by sample size\n",
    "    for metric in metrics:\n",
    "        metrics[metric].sort(key=lambda x: x['sample_size'])\n",
    "    \n",
    "    print(\"\\n\\nðŸ“Š SCALABILITY ANALYSIS BY METRIC\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Time budgets to analyze (seconds)\n",
    "    time_budgets = [1, 5, 10, 30, 60, 120, 300, 600]\n",
    "    optimal_configs = {}\n",
    "    \n",
    "    for metric_name, metric_results in metrics.items():\n",
    "        print(f\"\\nðŸŽ¯ {metric_name.upper().replace('_', ' ')}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Display results table\n",
    "        print(\"   Sample Size â”‚   Time (s) â”‚   Ratio â”‚  Success â”‚    Value    â”‚ Std Dev\")\n",
    "        print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "        \n",
    "        for result in metric_results:\n",
    "            size_str = f\"{result['sample_size']:,}\"\n",
    "            time_str = f\"{result['avg_computation_time']:.3f}\"\n",
    "            ratio_str = f\"{result['sample_ratio']:.4f}\"\n",
    "            success_str = f\"{result['num_successful_runs']}/3\" if result['num_successful_runs'] < 3 else \"âœ“\"\n",
    "            value_str = f\"{result['avg_metric_value']:.4f}\"\n",
    "            std_str = f\"{result['std_metric_value']:.4f}\"\n",
    "            \n",
    "            # Color coding based on time\n",
    "            if result['avg_computation_time'] > 60:\n",
    "                time_indicator = \"ðŸ”´\"\n",
    "            elif result['avg_computation_time'] > 10:\n",
    "                time_indicator = \"ðŸŸ¡\"\n",
    "            else:\n",
    "                time_indicator = \"ðŸŸ¢\"\n",
    "            \n",
    "            print(f\"{size_str:>13} â”‚ {time_str:>9} {time_indicator} â”‚ {ratio_str:>6} â”‚ {success_str:>7} â”‚ {value_str:>10} â”‚ {std_str:>6}\")\n",
    "        \n",
    "        # Find optimal configurations for different time budgets\n",
    "        optimal_configs[metric_name] = {}\n",
    "        \n",
    "        print(f\"\\nâ±ï¸  Optimal sample sizes by time budget:\")\n",
    "        for budget in time_budgets:\n",
    "            # Find largest viable sample within budget\n",
    "            viable = [r for r in metric_results \n",
    "                     if r['avg_computation_time'] <= budget and r['num_successful_runs'] >= 2]\n",
    "            \n",
    "            if viable:\n",
    "                optimal = max(viable, key=lambda x: x['sample_size'])\n",
    "                optimal_configs[metric_name][budget] = optimal\n",
    "                efficiency = optimal['sample_size'] / optimal['avg_computation_time']\n",
    "                print(f\"   {budget:>3.0f}s â†’ {optimal['sample_size']:>7,} nodes \"\n",
    "                      f\"({optimal['sample_ratio']:.3f}, {efficiency:,.0f} nodes/sec)\")\n",
    "            else:\n",
    "                print(f\"   {budget:>3.0f}s â†’ No viable option\")\n",
    "    \n",
    "    # Generate recommendations\n",
    "    print(f\"\\n\\nðŸ† RECOMMENDATIONS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Different use cases\n",
    "    use_cases = {\n",
    "        \"Quick Analysis\": {\"budget\": 5, \"description\": \"Fast exploratory analysis\"},\n",
    "        \"Standard Analysis\": {\"budget\": 30, \"description\": \"Balanced speed vs accuracy\"},\n",
    "        \"Detailed Analysis\": {\"budget\": 120, \"description\": \"High accuracy analysis\"},\n",
    "        \"Research Quality\": {\"budget\": 600, \"description\": \"Publication-ready results\"}\n",
    "    }\n",
    "    \n",
    "    for case_name, case_info in use_cases.items():\n",
    "        budget = case_info[\"budget\"]\n",
    "        print(f\"\\nðŸ“‹ {case_name} ({case_info['description']})\")\n",
    "        print(f\"Time Budget: {budget}s\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"Metric              â”‚ Sample Size â”‚  Ratio â”‚   Time â”‚ Coverage\")\n",
    "        print(\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "        \n",
    "        for metric_name in ['diameter', 'radius', 'avg_distance', 'global_efficiency']:\n",
    "            if metric_name in optimal_configs and budget in optimal_configs[metric_name]:\n",
    "                config = optimal_configs[metric_name][budget]\n",
    "                coverage = config['sample_ratio'] * 100\n",
    "                print(f\"{metric_name.replace('_', ' '):>18} â”‚ {config['sample_size']:>10,} â”‚ \"\n",
    "                      f\"{config['sample_ratio']:.4f} â”‚ {config['avg_computation_time']:>6.1f}s â”‚ \"\n",
    "                      f\"{coverage:>6.1f}%\")\n",
    "            else:\n",
    "                print(f\"{metric_name.replace('_', ' '):>18} â”‚ {'Not viable':>10} â”‚ {'---':>6} â”‚ {'---':>6} â”‚ {'---':>6}\")\n",
    "    \n",
    "    # Key insights\n",
    "    print(f\"\\n\\nðŸ’¡ KEY INSIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Compute scalability insights\n",
    "    for metric_name, metric_results in metrics.items():\n",
    "        if len(metric_results) < 2:\n",
    "            continue\n",
    "            \n",
    "        # Find the scaling pattern\n",
    "        small_samples = [r for r in metric_results if r['sample_size'] < 1000]\n",
    "        large_samples = [r for r in metric_results if r['sample_size'] > 5000]\n",
    "        \n",
    "        if small_samples and large_samples:\n",
    "            small_time = min(r['avg_computation_time'] for r in small_samples)\n",
    "            large_time = max(r['avg_computation_time'] for r in large_samples)\n",
    "            small_size = min(r['sample_size'] for r in small_samples)\n",
    "            large_size = max(r['sample_size'] for r in large_samples)\n",
    "            \n",
    "            time_ratio = large_time / small_time\n",
    "            size_ratio = large_size / small_size\n",
    "            \n",
    "            # Estimate computational complexity\n",
    "            if size_ratio > 1:\n",
    "                complexity_exp = round(math.log(time_ratio) / math.log(size_ratio), 1)\n",
    "                complexity_desc = f\"O(n^{complexity_exp})\" if complexity_exp > 1 else \"Sub-quadratic\"\n",
    "            else:\n",
    "                complexity_desc = \"Cannot determine\"\n",
    "            \n",
    "            print(f\"\\nðŸ” {metric_name.upper()}:\")\n",
    "            print(f\"   â€¢ Scales from {small_size:,} to {large_size:,} nodes\")\n",
    "            print(f\"   â€¢ Time increases {time_ratio:.1f}x for {size_ratio:.1f}x more nodes\")\n",
    "            print(f\"   â€¢ Estimated complexity: {complexity_desc}\")\n",
    "            \n",
    "            # Sweet spot recommendation\n",
    "            sweet_spot = None\n",
    "            for r in metric_results:\n",
    "                if 5 < r['avg_computation_time'] < 60:  # 5s to 1min sweet spot\n",
    "                    if sweet_spot is None or r['sample_size'] > sweet_spot['sample_size']:\n",
    "                        sweet_spot = r\n",
    "            \n",
    "            if sweet_spot:\n",
    "                coverage = sweet_spot['sample_ratio'] * 100\n",
    "                print(f\"   â€¢ Recommended sample: {sweet_spot['sample_size']:,} nodes \"\n",
    "                      f\"({coverage:.1f}% coverage, ~{sweet_spot['avg_computation_time']:.1f}s)\")\n",
    "    \n",
    "    print(f\"\\n\\nâš¡ PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"â€¢ Diameter and radius scale poorly - use sampling for large graphs\")\n",
    "    print(\"â€¢ Average distance is more tractable but still requires careful sampling\")  \n",
    "    print(\"â€¢ Global efficiency computation time varies significantly with graph structure\")\n",
    "    print(\"â€¢ For graphs with 94K+ nodes, sample sizes of 1K-10K provide good balance\")\n",
    "    print(\"â€¢ Consider approximate algorithms for very large graphs or tight time budgets\")\n",
    "    \n",
    "    return optimal_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90e6e7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—ï¸  GRAPH STRUCTURE\n",
      "================================================================================\n",
      "Total nodes: 94,675\n",
      "Total edges: 4,900,260\n",
      "Graph type: directed\n",
      "Max time threshold: 600.0s\n",
      "Experiment date: 2025-09-23T18:49:03.639972\n",
      "\n",
      "\n",
      "ðŸ“Š SCALABILITY ANALYSIS BY METRIC\n",
      "================================================================================\n",
      "\n",
      "ðŸŽ¯ DIAMETER\n",
      "------------------------------------------------------------\n",
      "   Sample Size â”‚   Time (s) â”‚   Ratio â”‚  Success â”‚    Value    â”‚ Std Dev\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "          100 â”‚     0.000 ðŸŸ¢ â”‚ 0.0011 â”‚       âœ“ â”‚     0.0000 â”‚ 0.0000\n",
      "          180 â”‚     0.000 ðŸŸ¢ â”‚ 0.0019 â”‚       âœ“ â”‚     1.3333 â”‚ 0.4714\n",
      "          324 â”‚     0.000 ðŸŸ¢ â”‚ 0.0034 â”‚       âœ“ â”‚     2.0000 â”‚ 0.0000\n",
      "          583 â”‚     0.000 ðŸŸ¢ â”‚ 0.0062 â”‚       âœ“ â”‚     2.0000 â”‚ 0.0000\n",
      "        1,049 â”‚     0.000 ðŸŸ¢ â”‚ 0.0111 â”‚       âœ“ â”‚     5.6667 â”‚ 2.0548\n",
      "        1,889 â”‚     0.012 ðŸŸ¢ â”‚ 0.0200 â”‚       âœ“ â”‚    11.6667 â”‚ 2.4944\n",
      "        3,401 â”‚     0.242 ðŸŸ¢ â”‚ 0.0359 â”‚       âœ“ â”‚     9.6667 â”‚ 0.4714\n",
      "        6,122 â”‚     4.931 ðŸŸ¢ â”‚ 0.0647 â”‚       âœ“ â”‚    10.0000 â”‚ 0.8165\n",
      "       10,000 â”‚    34.653 ðŸŸ¡ â”‚ 0.1056 â”‚     2/3 â”‚     8.0000 â”‚ 0.0000\n",
      "       15,000 â”‚   158.384 ðŸ”´ â”‚ 0.1584 â”‚     2/3 â”‚     8.5000 â”‚ 0.5000\n",
      "       20,000 â”‚   416.733 ðŸ”´ â”‚ 0.2112 â”‚     2/3 â”‚     8.0000 â”‚ 0.0000\n",
      "       25,000 â”‚   850.994 ðŸ”´ â”‚ 0.2641 â”‚     1/3 â”‚     6.0000 â”‚ 0.0000\n",
      "\n",
      "â±ï¸  Optimal sample sizes by time budget:\n",
      "     1s â†’   3,401 nodes (0.036, 14,054 nodes/sec)\n",
      "     5s â†’   6,122 nodes (0.065, 1,242 nodes/sec)\n",
      "    10s â†’   6,122 nodes (0.065, 1,242 nodes/sec)\n",
      "    30s â†’   6,122 nodes (0.065, 1,242 nodes/sec)\n",
      "    60s â†’  10,000 nodes (0.106, 289 nodes/sec)\n",
      "   120s â†’  10,000 nodes (0.106, 289 nodes/sec)\n",
      "   300s â†’  15,000 nodes (0.158, 95 nodes/sec)\n",
      "   600s â†’  20,000 nodes (0.211, 48 nodes/sec)\n",
      "\n",
      "ðŸŽ¯ RADIUS\n",
      "------------------------------------------------------------\n",
      "   Sample Size â”‚   Time (s) â”‚   Ratio â”‚  Success â”‚    Value    â”‚ Std Dev\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "          100 â”‚     0.000 ðŸŸ¢ â”‚ 0.0011 â”‚       âœ“ â”‚     1.0000 â”‚ 0.0000\n",
      "          180 â”‚     0.000 ðŸŸ¢ â”‚ 0.0019 â”‚       âœ“ â”‚     1.0000 â”‚ 0.0000\n",
      "          324 â”‚     0.000 ðŸŸ¢ â”‚ 0.0034 â”‚       âœ“ â”‚     1.0000 â”‚ 0.0000\n",
      "          583 â”‚     0.000 ðŸŸ¢ â”‚ 0.0062 â”‚       âœ“ â”‚     1.0000 â”‚ 0.0000\n",
      "        1,049 â”‚     0.000 ðŸŸ¢ â”‚ 0.0111 â”‚       âœ“ â”‚     2.3333 â”‚ 1.2472\n",
      "        1,889 â”‚     0.011 ðŸŸ¢ â”‚ 0.0200 â”‚       âœ“ â”‚     5.0000 â”‚ 0.8165\n",
      "        3,401 â”‚     0.341 ðŸŸ¢ â”‚ 0.0359 â”‚       âœ“ â”‚     5.6667 â”‚ 0.4714\n",
      "        6,122 â”‚     5.076 ðŸŸ¢ â”‚ 0.0647 â”‚       âœ“ â”‚     5.6667 â”‚ 0.4714\n",
      "       10,000 â”‚    34.942 ðŸŸ¡ â”‚ 0.1056 â”‚     2/3 â”‚     5.0000 â”‚ 0.0000\n",
      "       15,000 â”‚   152.393 ðŸ”´ â”‚ 0.1584 â”‚     2/3 â”‚     3.0000 â”‚ 0.0000\n",
      "       20,000 â”‚   395.931 ðŸ”´ â”‚ 0.2112 â”‚     2/3 â”‚     4.0000 â”‚ 0.0000\n",
      "       25,000 â”‚   894.172 ðŸ”´ â”‚ 0.2641 â”‚     1/3 â”‚     4.0000 â”‚ 0.0000\n",
      "\n",
      "â±ï¸  Optimal sample sizes by time budget:\n",
      "     1s â†’   3,401 nodes (0.036, 9,987 nodes/sec)\n",
      "     5s â†’   3,401 nodes (0.036, 9,987 nodes/sec)\n",
      "    10s â†’   6,122 nodes (0.065, 1,206 nodes/sec)\n",
      "    30s â†’   6,122 nodes (0.065, 1,206 nodes/sec)\n",
      "    60s â†’  10,000 nodes (0.106, 286 nodes/sec)\n",
      "   120s â†’  10,000 nodes (0.106, 286 nodes/sec)\n",
      "   300s â†’  15,000 nodes (0.158, 98 nodes/sec)\n",
      "   600s â†’  20,000 nodes (0.211, 51 nodes/sec)\n",
      "\n",
      "ðŸŽ¯ AVG DISTANCE\n",
      "------------------------------------------------------------\n",
      "   Sample Size â”‚   Time (s) â”‚   Ratio â”‚  Success â”‚    Value    â”‚ Std Dev\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "          100 â”‚     0.000 ðŸŸ¢ â”‚ 0.0011 â”‚       âœ“ â”‚     0.6667 â”‚ 0.4714\n",
      "          180 â”‚     0.000 ðŸŸ¢ â”‚ 0.0019 â”‚       âœ“ â”‚     1.2222 â”‚ 0.3143\n",
      "          324 â”‚     0.000 ðŸŸ¢ â”‚ 0.0034 â”‚       âœ“ â”‚     1.1111 â”‚ 0.1571\n",
      "          583 â”‚     0.000 ðŸŸ¢ â”‚ 0.0062 â”‚       âœ“ â”‚     1.7000 â”‚ 0.1414\n",
      "        1,049 â”‚     0.000 ðŸŸ¢ â”‚ 0.0111 â”‚       âœ“ â”‚     3.0159 â”‚ 0.9290\n",
      "        1,889 â”‚     0.006 ðŸŸ¢ â”‚ 0.0200 â”‚       âœ“ â”‚     3.9848 â”‚ 0.2096\n",
      "        3,401 â”‚     0.284 ðŸŸ¢ â”‚ 0.0359 â”‚       âœ“ â”‚     4.2861 â”‚ 0.1450\n",
      "        6,122 â”‚     3.900 ðŸŸ¢ â”‚ 0.0647 â”‚       âœ“ â”‚     3.6124 â”‚ 0.6269\n",
      "       10,000 â”‚    32.314 ðŸŸ¡ â”‚ 0.1056 â”‚     2/3 â”‚     3.4741 â”‚ 0.0037\n",
      "       15,000 â”‚   151.992 ðŸ”´ â”‚ 0.1584 â”‚     2/3 â”‚     3.1903 â”‚ 0.2553\n",
      "       20,000 â”‚   400.740 ðŸ”´ â”‚ 0.2112 â”‚     2/3 â”‚     3.0002 â”‚ 0.4213\n",
      "       25,000 â”‚   847.007 ðŸ”´ â”‚ 0.2641 â”‚     1/3 â”‚     2.8415 â”‚ 0.0000\n",
      "\n",
      "â±ï¸  Optimal sample sizes by time budget:\n",
      "     1s â†’   3,401 nodes (0.036, 11,955 nodes/sec)\n",
      "     5s â†’   6,122 nodes (0.065, 1,570 nodes/sec)\n",
      "    10s â†’   6,122 nodes (0.065, 1,570 nodes/sec)\n",
      "    30s â†’   6,122 nodes (0.065, 1,570 nodes/sec)\n",
      "    60s â†’  10,000 nodes (0.106, 309 nodes/sec)\n",
      "   120s â†’  10,000 nodes (0.106, 309 nodes/sec)\n",
      "   300s â†’  15,000 nodes (0.158, 99 nodes/sec)\n",
      "   600s â†’  20,000 nodes (0.211, 50 nodes/sec)\n",
      "\n",
      "ðŸŽ¯ GLOBAL EFFICIENCY\n",
      "------------------------------------------------------------\n",
      "   Sample Size â”‚   Time (s) â”‚   Ratio â”‚  Success â”‚    Value    â”‚ Std Dev\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "          100 â”‚     0.000 ðŸŸ¢ â”‚ 0.0011 â”‚       âœ“ â”‚     0.3333 â”‚ 0.4714\n",
      "          180 â”‚     0.000 ðŸŸ¢ â”‚ 0.0019 â”‚       âœ“ â”‚     0.6667 â”‚ 0.4714\n",
      "          324 â”‚     0.000 ðŸŸ¢ â”‚ 0.0034 â”‚       âœ“ â”‚     0.8426 â”‚ 0.1249\n",
      "          583 â”‚     0.000 ðŸŸ¢ â”‚ 0.0062 â”‚       âœ“ â”‚     0.7351 â”‚ 0.0725\n",
      "        1,049 â”‚     0.000 ðŸŸ¢ â”‚ 0.0111 â”‚       âœ“ â”‚     0.5667 â”‚ 0.1123\n",
      "        1,889 â”‚     0.010 ðŸŸ¢ â”‚ 0.0200 â”‚       âœ“ â”‚     0.2806 â”‚ 0.0088\n",
      "        3,401 â”‚     0.274 ðŸŸ¢ â”‚ 0.0359 â”‚       âœ“ â”‚     0.2636 â”‚ 0.0094\n",
      "        6,122 â”‚     4.863 ðŸŸ¢ â”‚ 0.0647 â”‚       âœ“ â”‚     0.2616 â”‚ 0.0060\n",
      "       10,000 â”‚    35.735 ðŸŸ¡ â”‚ 0.1056 â”‚     1/3 â”‚     0.3795 â”‚ 0.0000\n",
      "       15,000 â”‚   157.396 ðŸ”´ â”‚ 0.1584 â”‚     1/3 â”‚     0.3129 â”‚ 0.0000\n",
      "       20,000 â”‚   411.806 ðŸ”´ â”‚ 0.2112 â”‚     1/3 â”‚     0.3706 â”‚ 0.0000\n",
      "       25,000 â”‚   847.985 ðŸ”´ â”‚ 0.2641 â”‚     1/3 â”‚     0.3645 â”‚ 0.0000\n",
      "\n",
      "â±ï¸  Optimal sample sizes by time budget:\n",
      "     1s â†’   3,401 nodes (0.036, 12,390 nodes/sec)\n",
      "     5s â†’   6,122 nodes (0.065, 1,259 nodes/sec)\n",
      "    10s â†’   6,122 nodes (0.065, 1,259 nodes/sec)\n",
      "    30s â†’   6,122 nodes (0.065, 1,259 nodes/sec)\n",
      "    60s â†’   6,122 nodes (0.065, 1,259 nodes/sec)\n",
      "   120s â†’   6,122 nodes (0.065, 1,259 nodes/sec)\n",
      "   300s â†’   6,122 nodes (0.065, 1,259 nodes/sec)\n",
      "   600s â†’   6,122 nodes (0.065, 1,259 nodes/sec)\n",
      "\n",
      "\n",
      "ðŸ† RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Quick Analysis (Fast exploratory analysis)\n",
      "Time Budget: 5s\n",
      "--------------------------------------------------\n",
      "Metric              â”‚ Sample Size â”‚  Ratio â”‚   Time â”‚ Coverage\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "          diameter â”‚      6,122 â”‚ 0.0647 â”‚    4.9s â”‚    6.5%\n",
      "            radius â”‚      3,401 â”‚ 0.0359 â”‚    0.3s â”‚    3.6%\n",
      "      avg distance â”‚      6,122 â”‚ 0.0647 â”‚    3.9s â”‚    6.5%\n",
      " global efficiency â”‚      6,122 â”‚ 0.0647 â”‚    4.9s â”‚    6.5%\n",
      "\n",
      "ðŸ“‹ Standard Analysis (Balanced speed vs accuracy)\n",
      "Time Budget: 30s\n",
      "--------------------------------------------------\n",
      "Metric              â”‚ Sample Size â”‚  Ratio â”‚   Time â”‚ Coverage\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "          diameter â”‚      6,122 â”‚ 0.0647 â”‚    4.9s â”‚    6.5%\n",
      "            radius â”‚      6,122 â”‚ 0.0647 â”‚    5.1s â”‚    6.5%\n",
      "      avg distance â”‚      6,122 â”‚ 0.0647 â”‚    3.9s â”‚    6.5%\n",
      " global efficiency â”‚      6,122 â”‚ 0.0647 â”‚    4.9s â”‚    6.5%\n",
      "\n",
      "ðŸ“‹ Detailed Analysis (High accuracy analysis)\n",
      "Time Budget: 120s\n",
      "--------------------------------------------------\n",
      "Metric              â”‚ Sample Size â”‚  Ratio â”‚   Time â”‚ Coverage\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "          diameter â”‚     10,000 â”‚ 0.1056 â”‚   34.7s â”‚   10.6%\n",
      "            radius â”‚     10,000 â”‚ 0.1056 â”‚   34.9s â”‚   10.6%\n",
      "      avg distance â”‚     10,000 â”‚ 0.1056 â”‚   32.3s â”‚   10.6%\n",
      " global efficiency â”‚      6,122 â”‚ 0.0647 â”‚    4.9s â”‚    6.5%\n",
      "\n",
      "ðŸ“‹ Research Quality (Publication-ready results)\n",
      "Time Budget: 600s\n",
      "--------------------------------------------------\n",
      "Metric              â”‚ Sample Size â”‚  Ratio â”‚   Time â”‚ Coverage\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "          diameter â”‚     20,000 â”‚ 0.2112 â”‚  416.7s â”‚   21.1%\n",
      "            radius â”‚     20,000 â”‚ 0.2112 â”‚  395.9s â”‚   21.1%\n",
      "      avg distance â”‚     20,000 â”‚ 0.2112 â”‚  400.7s â”‚   21.1%\n",
      " global efficiency â”‚      6,122 â”‚ 0.0647 â”‚    4.9s â”‚    6.5%\n",
      "\n",
      "\n",
      "ðŸ’¡ KEY INSIGHTS\n",
      "================================================================================\n",
      "\n",
      "ðŸ” DIAMETER:\n",
      "   â€¢ Scales from 100 to 25,000 nodes\n",
      "   â€¢ Time increases 35574679.3x for 250.0x more nodes\n",
      "   â€¢ Estimated complexity: O(n^3.1)\n",
      "   â€¢ Recommended sample: 10,000 nodes (10.6% coverage, ~34.7s)\n",
      "\n",
      "ðŸ” RADIUS:\n",
      "   â€¢ Scales from 100 to 25,000 nodes\n",
      "   â€¢ Time increases 33787645.5x for 250.0x more nodes\n",
      "   â€¢ Estimated complexity: O(n^3.1)\n",
      "   â€¢ Recommended sample: 10,000 nodes (10.6% coverage, ~34.9s)\n",
      "\n",
      "ðŸ” AVG_DISTANCE:\n",
      "   â€¢ Scales from 100 to 25,000 nodes\n",
      "   â€¢ Time increases 32005458.9x for 250.0x more nodes\n",
      "   â€¢ Estimated complexity: O(n^3.1)\n",
      "   â€¢ Recommended sample: 10,000 nodes (10.6% coverage, ~32.3s)\n",
      "\n",
      "ðŸ” GLOBAL_EFFICIENCY:\n",
      "   â€¢ Scales from 100 to 25,000 nodes\n",
      "   â€¢ Time increases 45598806.9x for 250.0x more nodes\n",
      "   â€¢ Estimated complexity: O(n^3.2)\n",
      "   â€¢ Recommended sample: 10,000 nodes (10.6% coverage, ~35.7s)\n",
      "\n",
      "\n",
      "âš¡ PERFORMANCE SUMMARY\n",
      "================================================================================\n",
      "â€¢ Diameter and radius scale poorly - use sampling for large graphs\n",
      "â€¢ Average distance is more tractable but still requires careful sampling\n",
      "â€¢ Global efficiency computation time varies significantly with graph structure\n",
      "â€¢ For graphs with 94K+ nodes, sample sizes of 1K-10K provide good balance\n",
      "â€¢ Consider approximate algorithms for very large graphs or tight time budgets\n"
     ]
    }
   ],
   "source": [
    "# Load JSON file\n",
    "with open(\"../community_detection/output/sample_size_optimization_results.json\", \"r\") as f:\n",
    "    sample_data = json.load(f)\n",
    "\n",
    "# Pass data to your analysis function\n",
    "result = analyze_sample_size_optimization(sample_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
